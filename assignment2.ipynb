{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 DSC 102 2020 WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will conduct data engineering for the Amazon dataset. The extracted features will be used for your next assignment, where you train a model (or models) to predict user ratings for a product.\n",
    "\n",
    "We will be using Apache Spark for this assignment. The default Spark API will be DataFrame, as it is now the recommended choice over the RDD API. That being said, please feel free to switch back to the RDD API if you see it as a better fit for the task. We provide you an option to request RDD format to start with. Also you can switch between DataFrame and RDD in your solution. \n",
    "\n",
    "Another newer API is Koalas, which is also avaliable. However, it has constraints and is not applicable to most tasks. Refer to the PA statement for detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:47:30.022030Z",
     "start_time": "2020-02-03T18:47:30.019499Z"
    }
   },
   "outputs": [],
   "source": [
    "PID = 'a14664383' # your pid, for instance: 'a43223333'\n",
    "INPUT_FORMAT = 'dataframe' # choose a format of your input data, valid options: 'dataframe', 'rdd', 'koalas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:50:16.780690Z",
     "start_time": "2020-02-03T18:47:30.263681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets ...Done\n"
     ]
    }
   ],
   "source": [
    "# Boiler plates, do not modify\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "from utilities import data_cat\n",
    "from pa2_main import PA2Executor\n",
    "import time\n",
    "if INPUT_FORMAT == 'dataframe':\n",
    "    import pyspark.ml as M\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "if INPUT_FORMAT == 'koalas':\n",
    "    import databricks.koalas as ks\n",
    "elif INPUT_FORMAT == 'rdd':\n",
    "    import pyspark.mllib as M\n",
    "    from pyspark.mllib.feature import Word2Vec\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--deploy-mode client \\\n",
    "pyspark-shell'\n",
    "\n",
    "class args:\n",
    "    review_filename = data_cat.review_filename\n",
    "    product_filename = data_cat.product_filename\n",
    "    product_processed_filename = data_cat.product_processed_filename\n",
    "    ml_features_train_filename = data_cat.ml_features_train_filename\n",
    "    ml_features_test_filename = data_cat.ml_features_test_filename\n",
    "    output_root = '/home/{}-pa2/test_results'.format(PID)\n",
    "    test_results_root = data_cat.test_results_root\n",
    "    pid = PID\n",
    "\n",
    "pa2 = PA2Executor(args, input_format=INPUT_FORMAT)\n",
    "data_io = pa2.data_io\n",
    "data_dict = pa2.data_dict  \n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.353249Z",
     "start_time": "2020-01-26T20:45:52.343036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import your own dependencies\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the part_1 datasets to memory and de-cache part_2 datasets. \n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task0: warm up \n",
    "This task is provided for you to get familiar with Spark API. We will use the dataframe API to demonstrate. Solution is given to you and this task won't be graded.\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html for API guide.\n",
    "\n",
    "The task is to implement the function below. Given the ```product_data``` table:\n",
    "1. Take and print five rows.\n",
    "\n",
    "1. Select only the ```asin``` column, then print five rows of it.\n",
    "\n",
    "1. Select the row where ```asin = B00I8KEOTM``` and print it.\n",
    "\n",
    "1. Count the total number of rows.\n",
    "\n",
    "1. Calculate the mean ```price```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after your operations\n",
    "     | -- mean_price: float -- mean value of column price\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.368918Z",
     "start_time": "2020-01-26T20:45:52.355018Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_0(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    product_data.show(5)\n",
    "    product_data[['asin']].show(5)\n",
    "    product_data.where(F.col('asin') == 'B00I8KEOTM').show()\n",
    "    count_rows = product_data.count()\n",
    "    mean_price = product_data.select(F.avg(F.col('price'))).head()[0]\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmaticly. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {'count_total': None, 'mean_price': None}\n",
    "    # Modify res:\n",
    "\n",
    "    res['count_total'] = count_rows\n",
    "    res['mean_price'] = mean_price\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:54.222111Z",
     "start_time": "2020-01-26T20:45:52.370377Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|      asin|           salesRank|          categories|               title|price|             related|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|B00I8HVV6E|[Home &amp; Kitch...|[[Home & Kitchen,...|Intelligent Desig...|27.99|[also_viewed -> [...|\n",
      "|B00I8KEOTM|                null|[[Apps for Androi...|                null| null|[also_viewed -> [...|\n",
      "|B00I8KCW4G|[Clothing -> 2233...|[[Clothing, Shoes...|eShakti Women's P...|41.95|[also_viewed -> [...|\n",
      "|B00I8JKCQW|[Clothing -> 1405...|[[Clothing, Shoes...|Lady Slimming Mid...| null|[also_viewed -> [...|\n",
      "|B00I8JKI8E|[Home &amp; Kitch...|[[Clothing, Shoes...|3 Tier Bangle Bra...|24.99|[also_viewed -> [...|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      asin|\n",
      "+----------+\n",
      "|B00I8HVV6E|\n",
      "|B00I8KEOTM|\n",
      "|B00I8KCW4G|\n",
      "|B00I8JKCQW|\n",
      "|B00I8JKI8E|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|      asin|salesRank|          categories|title|price|             related|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|B00I8KEOTM|     null|[[Apps for Androi...| null| null|[also_viewed -> [...|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "\n",
      "tests for task_0 --------------------------------------------------------------\n",
      "2/2 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if INPUT_FORMAT == 'dataframe':\n",
    "    res = task_0(data_io, data_dict['product'])\n",
    "    pa2.tests.test(res, 'task_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:30:44.034299Z",
     "start_time": "2019-12-10T21:30:44.012787Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_1 assignment2.py\n",
    "def task_1(data_io, review_data, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # old method time: 120.27334499359131s\n",
    "    # combine method time: 100.53635740280151\n",
    "    #p_asin = p.select(p['asin'])\n",
    "    #combine = p_asin.join(agg_df, on='asin', how='left')\n",
    "    \n",
    "    # step 1&2\n",
    "    # use agg to calculate the mean and count\n",
    "    gr = review_data.groupBy(review_data.asin)\n",
    "    agg_df = gr.agg({'overall':'mean', 'asin':'count'})\n",
    "    # rename the table\n",
    "    agg_df = agg_df.withColumnRenamed('avg(overall)', 'meanRating').withColumnRenamed('count(asin)', 'countRating')\n",
    "    # join the agg_df to product_data\n",
    "    p_asin = product_data.select(product_data['asin'])\n",
    "    product_data = p_asin.join(agg_df, on='asin', how='left')\n",
    "    \n",
    "    # step 3\n",
    "    count_row = product_data.count()\n",
    "    \n",
    "    mean_mr = product_data.select(F.avg(F.col('meanRating'))).head()[0]\n",
    "    var_mr = product_data.select(F.var_samp(F.col('meanRating'))).head()[0]\n",
    "    numNulls_mr = product_data.filter(product_data['meanRating'].isNull()).count()\n",
    "    \n",
    "    mean_cr = product_data.select(F.avg(F.col('countRating'))).head()[0]\n",
    "    var_cr = product_data.select(F.var_samp(F.col('countRating'))).head()[0]\n",
    "    numNulls_cr = product_data.filter(product_data['countRating'].isNull()).count()\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmaticly. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanRating': None,\n",
    "        'variance_meanRating': None,\n",
    "        'numNulls_meanRating': None,\n",
    "        'mean_countRating': None,\n",
    "        'variance_countRating': None,\n",
    "        'numNulls_countRating': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res['count_total'] = count_row\n",
    "    res['mean_meanRating'] = mean_mr\n",
    "    res['variance_meanRating'] = var_mr\n",
    "    res['numNulls_meanRating'] = numNulls_mr\n",
    "    res['mean_countRating'] = mean_cr\n",
    "    res['variance_countRating'] = var_cr\n",
    "    res['numNulls_countRating'] = numNulls_cr\n",
    "\n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_1')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:04.214179Z",
     "start_time": "2019-12-09T22:18:39.293699Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_meanRating': 4.1513144826254775, 'variance_meanRating': 1.1297362704403955, 'numNulls_meanRating': 1973780, 'mean_countRating': 7.444083463202534, 'variance_countRating': 2647.0014292689057, 'numNulls_countRating': 1973780}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_1 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countRating ... Pass\n",
      "Test 3/7 : mean_meanRating ... Pass\n",
      "Test 4/7 : numNulls_countRating ... Pass\n",
      "Test 5/7 : numNulls_meanRating ... Pass\n",
      "Test 6/7 : variance_countRating ... Pass\n",
      "Test 7/7 : variance_meanRating ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_1(data_io, data_dict['review'], data_dict['product'])\n",
    "pa2.tests.test(res, 'task_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:16.942833Z",
     "start_time": "2019-12-10T21:31:16.925378Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_2 assignment2.py\n",
    "def task_2(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    salesRank_column = 'salesRank'\n",
    "    categories_column = 'categories'\n",
    "    asin_column = 'asin'\n",
    "    # Outputs:\n",
    "    category_column = 'category'\n",
    "    bestSalesCategory_column = 'bestSalesCategory'\n",
    "    bestSalesRank_column = 'bestSalesRank'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # TODO: can we use alias?\n",
    "    p = product_data.alias('p')\n",
    "\n",
    "    # step 1\n",
    "    p = p.withColumn('category', F.flatten(p.categories))\n",
    "    p = p.withColumn('category', p['category'].getItem(0))\n",
    "    p = p.replace({'': None}, subset=['category'])\n",
    "    \n",
    "    # step 2\n",
    "    p = p.withColumn('bestSalesCategory', F.map_keys(F.col('salesRank')).getItem(0))\n",
    "    p = p.withColumn('bestSalesRank', F.map_values(F.col('salesRank')).getItem(0))\n",
    "    \n",
    "    product_data = p\n",
    "    \n",
    "    # step 3\n",
    "    cnt_total = product_data.count()\n",
    "    mean_bsr = product_data.select(F.avg(F.col('bestSalesRank'))).head()[0]\n",
    "    var_bsr = product_data.select(F.var_samp(F.col('bestSalesRank'))).head()[0]\n",
    "    numNulls_cat = product_data.filter(product_data['category'].isNull()).count()\n",
    "    cntDis_cat = product_data.select(F.countDistinct(product_data.category)).head()[0]\n",
    "    numNulls_bsc = product_data.filter(product_data['bestSalesCategory'].isNull()).count()\n",
    "    cntDis_bsc = product_data.select(F.countDistinct(product_data['bestSalesCategory'])).head()[0]\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_bestSalesRank': None,\n",
    "        'variance_bestSalesRank': None,\n",
    "        'numNulls_category': None,\n",
    "        'countDistinct_category': None,\n",
    "        'numNulls_bestSalesCategory': None,\n",
    "        'countDistinct_bestSalesCategory': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res['count_total'] = cnt_total\n",
    "    res['mean_bestSalesRank'] = mean_bsr\n",
    "    res['variance_bestSalesRank'] = var_bsr\n",
    "    res['numNulls_category'] = numNulls_cat\n",
    "    res['countDistinct_category'] = cntDis_cat\n",
    "    res['numNulls_bestSalesCategory'] = numNulls_bsc\n",
    "    res['countDistinct_bestSalesCategory'] = cntDis_bsc\n",
    "\n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_2')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:19.308187Z",
     "start_time": "2019-12-09T22:19:04.274345Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_bestSalesRank': 1102421.3114573301, 'variance_bestSalesRank': 3616424023407.04, 'numNulls_category': 270169, 'countDistinct_category': 82, 'numNulls_bestSalesCategory': 2480772, 'countDistinct_bestSalesCategory': 33}\n",
      "tests for task_2 --------------------------------------------------------------\n",
      "Test 1/7 : countDistinct_bestSalesCategory ... Pass\n",
      "Test 2/7 : countDistinct_category ... Pass\n",
      "Test 3/7 : count_total ... Pass\n",
      "Test 4/7 : mean_bestSalesRank ... Pass\n",
      "Test 5/7 : numNulls_bestSalesCategory ... Pass\n",
      "Test 6/7 : numNulls_category ... Pass\n",
      "Test 7/7 : variance_bestSalesRank ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_2(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:26.542481Z",
     "start_time": "2019-12-10T21:31:26.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_3 assignment2.py\n",
    "def task_3(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    price_column = 'price'\n",
    "    attribute = 'also_viewed'\n",
    "    related_column = 'related'\n",
    "    # Outputs:\n",
    "    meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "    countAlsoViewed_column = 'countAlsoViewed'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    price = product_data.select('asin', 'price')\n",
    "    price = price.withColumnRenamed('asin', 'priceAsin')\n",
    "    p = product_data.alias('p')\n",
    "    p = p.select('asin', 'related')\n",
    "    \n",
    "\n",
    "    # step 1\n",
    "    p = p.withColumn('also_viewed', p['related']['also_viewed'])\n",
    "    expl = p.select('asin', F.explode_outer(p['also_viewed']))\n",
    "    expl = expl.withColumnRenamed('col', 'avAsin')\n",
    "    joined = expl.join(price, expl['avAsin'] == price['priceAsin'], how='left')\n",
    "    \n",
    "    gr = joined.groupBy(joined.asin)\n",
    "    agg_df = gr.agg({'price':'mean'})\n",
    "    p1 = agg_df.withColumnRenamed('avg(price)', 'meanPriceAlsoViewed')\n",
    "    \n",
    "    # step 2: add length column\n",
    "    p2 = p.withColumn('countAlsoViewed', F.size(p['also_viewed']))\n",
    "    p2 = p2.replace({-1: None}, subset=['countAlsoViewed'])\n",
    "\n",
    "    # step 3\n",
    "    cnt_total = p1.count()\n",
    "    mean_mpav = p1.select(F.avg(F.col('meanPriceAlsoViewed'))).head()[0]\n",
    "    var_mpav = p1.select(F.var_samp(F.col('meanPriceAlsoViewed'))).head()[0]\n",
    "    numNulls_mpav = p1.filter(p1['meanPriceAlsoViewed'].isNull()).count()\n",
    "    mean_cav = p2.select(F.avg(F.col('countAlsoViewed'))).head()[0]\n",
    "    var_cav = p2.select(F.var_samp(F.col('countAlsoViewed'))).head()[0]\n",
    "    numNulls_cav = p2.filter(p2['countAlsoViewed'].isNull()).count()\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanPriceAlsoViewed': None,\n",
    "        'variance_meanPriceAlsoViewed': None,\n",
    "        'numNulls_meanPriceAlsoViewed': None,\n",
    "        'mean_countAlsoViewed': None,\n",
    "        'variance_countAlsoViewed': None,\n",
    "        'numNulls_countAlsoViewed': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res[\"count_total\"] = cnt_total\n",
    "    \n",
    "    res[\"mean_meanPriceAlsoViewed\"] =  mean_mpav\n",
    "    res['variance_meanPriceAlsoViewed'] =  var_mpav\n",
    "    res['numNulls_meanPriceAlsoViewed'] =  numNulls_mpav\n",
    "    res['mean_countAlsoViewed'] = mean_cav\n",
    "    res['variance_countAlsoViewed'] = var_cav\n",
    "    res['numNulls_countAlsoViewed'] =  numNulls_cav\n",
    "    \n",
    "    # TODO: delete this!\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_3')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:41.442745Z",
     "start_time": "2019-12-09T22:19:19.358780Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_meanPriceAlsoViewed': 45.285018283030176, 'variance_meanPriceAlsoViewed': 5135.311790051084, 'numNulls_meanPriceAlsoViewed': 5835773, 'mean_countAlsoViewed': 31.561228904913023, 'variance_countAlsoViewed': 562.8028467039483, 'numNulls_countAlsoViewed': 5408566}\n",
      "tests for task_3 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countAlsoViewed ... Pass\n",
      "Test 3/7 : mean_meanPriceAlsoViewed ... Pass\n",
      "Test 4/7 : numNulls_countAlsoViewed ... Pass\n",
      "Test 5/7 : numNulls_meanPriceAlsoViewed ... Pass\n",
      "Test 6/7 : variance_countAlsoViewed ... Pass\n",
      "Test 7/7 : variance_meanPriceAlsoViewed ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_3(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:39.503390Z",
     "start_time": "2019-12-10T21:31:39.484724Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_4 assignment2.py\n",
    "def task_4(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    price_column = 'price'\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    meanImputedPrice_column = 'meanImputedPrice'\n",
    "    medianImputedPrice_column = 'medianImputedPrice'\n",
    "    unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # TODO: can we use alias?\n",
    "    p = product_data.alias('p')\n",
    "    \n",
    "    # Step 1\n",
    "    p = p.withColumn('price', p['price'].cast('float'))\n",
    "    mean_price = p.select(F.avg(F.col('price'))).head()[0]\n",
    "    p = p.withColumn('meanImputedPrice', p['price'])\n",
    "\n",
    "    # Step 2\n",
    "    median_price = p.approxQuantile('price', [0.5], 0.001)[0]\n",
    "    p = p.withColumn('medianImputedPrice', p['price'])\n",
    "    \n",
    "    # Step 3\n",
    "    p = p.withColumn('unknownImputedTitle', p.title)\n",
    "    p = p.replace({'':'unknown'}, subset=['unknownImputedTitle'])\n",
    "\n",
    "    # fill nulls\n",
    "    p = p.fillna({'medianImputedPrice': median_price, \n",
    "                  'meanImputedPrice': mean_price,\n",
    "                  'unknownImputedTitle': 'unknown'})\n",
    "    \n",
    "    product_data = p\n",
    "    \n",
    "    # Step 4\n",
    "    cnt_total = product_data.count()\n",
    "    mean_mip = product_data.select(F.avg(F.col('meanImputedPrice'))).head()[0]\n",
    "    var_mip = product_data.select(F.var_samp(F.col('meanImputedPrice'))).head()[0]\n",
    "    numNulls_mip = product_data.filter(product_data['meanImputedPrice'].isNull()).count()\n",
    "    \n",
    "    mean_meip = product_data.select(F.avg(F.col('medianImputedPrice'))).head()[0]\n",
    "    var_meip = product_data.select(F.var_samp(F.col('medianImputedPrice'))).head()[0]\n",
    "    numNulls_meip = product_data.filter(product_data['medianImputedPrice'].isNull()).count()\n",
    "    numUnk = product_data.filter(product_data['unknownImputedTitle'] == 'unknown').count()\n",
    "    \n",
    "\n",
    "    #\"mean_medianImputedPrice\":27.815470873162305,\n",
    "    # 0 = 14.989999771118164\n",
    "    # 0.001 = 14.989999771118164\n",
    "    # 0.0001 = 14.989999771118164\n",
    "    # 0.25?\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanImputedPrice': None,\n",
    "        'variance_meanImputedPrice': None,\n",
    "        'numNulls_meanImputedPrice': None,\n",
    "        'mean_medianImputedPrice': None,\n",
    "        'variance_medianImputedPrice': None,\n",
    "        'numNulls_medianImputedPrice': None,\n",
    "        'numUnknowns_unknownImputedTitle': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = cnt_total\n",
    "    res['mean_meanImputedPrice'] = mean_mip\n",
    "    res['variance_meanImputedPrice'] = var_mip\n",
    "    res['numNulls_meanImputedPrice'] = numNulls_mip\n",
    "    res['mean_medianImputedPrice'] = mean_meip\n",
    "    res['variance_medianImputedPrice'] = var_meip\n",
    "    res['numNulls_medianImputedPrice'] = numNulls_meip\n",
    "    res['numUnknowns_unknownImputedTitle'] = numUnk\n",
    "    \n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_4')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:47.953226Z",
     "start_time": "2019-12-09T22:20:41.523379Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_meanImputedPrice': 34.93735571858629, 'variance_meanImputedPrice': 3265.3113437199545, 'numNulls_meanImputedPrice': 0, 'mean_medianImputedPrice': 27.81547087316232, 'variance_medianImputedPrice': 3356.6528865054042, 'numNulls_medianImputedPrice': 0, 'numUnknowns_unknownImputedTitle': 1432648}\n",
      "tests for task_4 --------------------------------------------------------------\n",
      "Test 1/8 : count_total ... Pass\n",
      "Test 2/8 : mean_meanImputedPrice ... Pass\n",
      "Test 3/8 : mean_medianImputedPrice ... Pass\n",
      "Test 4/8 : numNulls_meanImputedPrice ... Pass\n",
      "Test 5/8 : numNulls_medianImputedPrice ... Pass\n",
      "Test 6/8 : numUnknowns_unknownImputedTitle ... Pass\n",
      "Test 7/8 : variance_meanImputedPrice ... Pass\n",
      "Test 8/8 : variance_medianImputedPrice ... Pass\n",
      "8/8 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_4(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:29.284661Z",
     "start_time": "2019-12-10T21:32:29.267237Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_5 assignment2.py\n",
    "def task_5(data_io, product_processed_data, word_0, word_1, word_2):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    titleArray_column = 'titleArray'\n",
    "    titleVector_column = 'titleVector'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    p = product_processed_data.alias('p')\n",
    "    \n",
    "    # step 1\n",
    "    p = p.withColumn('titleArray', F.lower(p['title']))\n",
    "    p = p.withColumn('titleArray', F.split(p['titleArray'], '\\s'))\n",
    "    \n",
    "    # step 2\n",
    "    w2v = M.feature.Word2Vec(minCount=100, vectorSize=16, seed=SEED, \n",
    "                             numPartitions=4, inputCol='titleArray', outputCol='vec')\n",
    "    model = w2v.fit(p)\n",
    "    \n",
    "    \n",
    "    product_processed_data_output = p\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'size_vocabulary': None,\n",
    "        'word_0_synonyms': [(None, None), ],\n",
    "        'word_1_synonyms': [(None, None), ],\n",
    "        'word_2_synonyms': [(None, None), ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = product_processed_data_output.count()\n",
    "    res['size_vocabulary'] = model.getVectors().count()\n",
    "    for name, word in zip(\n",
    "        ['word_0_synonyms', 'word_1_synonyms', 'word_2_synonyms'],\n",
    "        [word_0, word_1, word_2]\n",
    "    ):\n",
    "        res[name] = model.findSynonymsArray(word, 10)\n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_5')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:26:05.015529Z",
     "start_time": "2019-12-09T22:20:47.999834Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'size_vocabulary': 42457, 'word_0_synonyms': [('mozart:', 0.9652077555656433), ('suites', 0.9600690603256226), ('sonatas', 0.9555727243423462), ('oboe', 0.9510754346847534), ('orchestra', 0.9495903849601746), ('brahms', 0.9470160603523254), ('orchestral', 0.9435944557189941), ('concerto,', 0.9435223340988159), ('orchestra,', 0.9414055347442627), ('etudes', 0.9398797750473022)], 'word_1_synonyms': [('shrimp', 0.9786536693572998), ('yogurt', 0.978205680847168), ('seasoning', 0.9746150374412537), ('seasoned', 0.9745858311653137), ('spice', 0.9742987155914307), ('barley', 0.9725334048271179), ('bites,', 0.9722008109092712), ('roasted', 0.9718379974365234), ('gluten', 0.97142493724823), ('lentil', 0.9708153009414673)], 'word_2_synonyms': [('notebook', 0.9813675284385681), ('g570', 0.965702474117279), ('lenovo', 0.9624518752098083), ('ultrabook', 0.961560845375061), ('17.3-inch', 0.9614698886871338), ('netbook', 0.9583598375320435), ('aspire', 0.9573705792427063), ('11.6-inch', 0.9552180767059326), ('chromebook', 0.9551472663879395), ('g60', 0.9536992311477661)]}\n",
      "tests for task_5 --------------------------------------------------------------\n",
      "Test 1/8 : count_total ... Pass\n",
      "Test 2/8 : size_vocabulary ... Pass\n",
      "Test 3/8 : word_0_synonyms-length ... Pass\n",
      "Test 4/8 : word_0_synonyms-correctness ... Pass\n",
      "Test 5/8 : word_1_synonyms-length ... Pass\n",
      "Test 6/8 : word_1_synonyms-correctness ... Pass\n",
      "Test 7/8 : word_2_synonyms-length ... Pass\n",
      "Test 8/8 : word_2_synonyms-correctness ... Pass\n",
      "5/5 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_5(data_io, data_dict['product_processed'], 'piano', 'rice', 'laptop')\n",
    "pa2.tests.test(res, 'task_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:39.991460Z",
     "start_time": "2019-12-10T21:32:39.974136Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_6 assignment2.py\n",
    "def task_6(data_io, product_processed_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    category_column = 'category'\n",
    "    # Outputs:\n",
    "    categoryIndex_column = 'categoryIndex'\n",
    "    categoryOneHot_column = 'categoryOneHot'\n",
    "    categoryPCA_column = 'categoryPCA'\n",
    "    # -------------------------------------------------------------------------    \n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    p = product_processed_data.alias('p')\n",
    "    \n",
    "    # step 1\n",
    "    # convert category column to numerical indices\n",
    "    indx = M.feature.StringIndexer(inputCol = 'category', outputCol='indexed')\n",
    "    indx_model = indx.fit(p)\n",
    "    p = indx_model.transform(p)\n",
    "    # one-hot encoding\n",
    "    ohe = M.feature.OneHotEncoderEstimator(inputCols=['indexed'], \n",
    "                                          outputCols=['categoryOneHot'], dropLast=False)\n",
    "    ohe_model = ohe.fit(p)\n",
    "    p = ohe_model.transform(p)\n",
    "    \n",
    "    # step 2\n",
    "    pca = M.feature.PCA(inputCol='categoryOneHot', outputCol='categoryPCA', k=15)\n",
    "    pca_model = pca.fit(p)\n",
    "    p = pca_model.transform(p)\n",
    "    \n",
    "    # step 4\n",
    "    cnt_total = p.count()\n",
    "    summ = Summarizer.metrics('mean')\n",
    "    mean_onehot = p.select(summ.summary(p.categoryOneHot)).head()[0][0]\n",
    "    mean_onehot = mean_onehot.values.tolist()\n",
    "    mean_pca = p.select(summ.summary(p.categoryPCA)).head()[0][0]\n",
    "    mean_pca = mean_pca.values.tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'meanVector_categoryOneHot': [None, ],\n",
    "        'meanVector_categoryPCA': [None, ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res['count_total'] = cnt_total\n",
    "    res['meanVector_categoryOneHot'] = mean_onehot\n",
    "    res['meanVector_categoryPCA'] = mean_pca\n",
    "    \n",
    "    # delete this\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_6')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:29:57.717617Z",
     "start_time": "2019-12-09T22:29:51.132434Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'meanVector_categoryOneHot': [0.25131601272534465, 0.1522659597030753, 0.05620243902439025, 0.05254252386002121, 0.05214347826086956, 0.04633690349946978, 0.0367677624602333, 0.03557656415694592, 0.03510593849416755, 0.028671261930010603, 0.028649946977730645, 0.02846967126193001, 0.02781728525980912, 0.02748356309650053, 0.020665323435843055, 0.01821325556733828, 0.014239766702014846, 0.012449946977730646, 0.011721633085896077, 0.01154305408271474, 0.0075626723223753975, 0.007260021208907741, 0.006526405090137858, 0.005181760339342524, 0.005004878048780487, 0.003249946977730647, 0.002560445387062566, 0.001878897136797455, 0.0012358430540827148, 0.0010625662778366914, 0.0008027571580063626, 0.0007919406150583245, 0.0006977730646871687, 0.0006583244962884412, 0.0006481442205726405, 0.0005278897136797455, 0.0004906680805938494, 0.000471898197242842, 0.000436373276776246, 0.00041823966065747615, 0.00040943796394485684, 0.00039777306468716863, 0.00037380699893955463, 0.000351643690349947, 0.00026691410392364794, 0.00024422057264050903, 0.00023944856839872746, 0.0002129374337221633, 0.00020424178154825027, 0.0002009544008483563, 0.00018048780487804877, 0.00017730646871686107, 0.00015662778366914103, 0.00015238600212089077, 0.00013117709437963945, 9.692470837751856e-05, 9.544008483563096e-05, 7.094379639448568e-05, 6.256627783669141e-05, 5.111346765641569e-05, 4.899257688229056e-05, 4.32661717921527e-05, 3.8282078472958644e-05, 3.340402969247084e-05, 3.297985153764581e-05, 3.022269353128314e-05, 1.6118769883351007e-05, 1.1452810180275715e-05, 6.256627783669141e-06, 3.499469777306469e-06, 2.8632025450689288e-06, 2.226935312831389e-06, 1.166489925768823e-06, 6.362672322375398e-07, 5.302226935312832e-07, 4.241781548250265e-07, 4.241781548250265e-07, 3.181336161187699e-07, 2.1208907741251325e-07, 2.1208907741251325e-07, 2.1208907741251325e-07, 1.0604453870625663e-07, 1.0604453870625663e-07], 'meanVector_categoryPCA': [-0.14997179233961794, -0.17518548836257788, 0.016545381493865943, -0.0026920098157592965, 0.03340161285017093, -0.053650029126477046, 0.009932691878126636, -0.00464979872562332, 0.040034786750453195, -0.0002622447858376596, -0.001857824285841033, 0.006117732894713227, -0.0037125132867396366, 0.03721883183864361, -0.030528362580817727]}\n",
      "tests for task_6 --------------------------------------------------------------\n",
      "Test 1/9 : count_total ... Pass\n",
      "Test 2/9 : meanVector_categoryOneHot-length ... Pass\n",
      "Test 3/9 : meanVector_categoryOneHot-sum ... Pass\n",
      "Test 4/9 : meanVector_categoryOneHot-mean ... Pass\n",
      "Test 5/9 : meanVector_categoryOneHot-variance ... Pass\n",
      "Test 6/9 : meanVector_categoryPCA-length ... Pass\n",
      "Test 7/9 : meanVector_categoryPCA-sum ... Pass\n",
      "Test 8/9 : meanVector_categoryPCA-mean ... Pass\n",
      "Test 9/9 : meanVector_categoryPCA-variance ... Pass\n",
      "9/9 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_6(data_io, data_dict['product_processed'])\n",
    "pa2.tests.test(res, 'task_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:23:18.882119Z",
     "start_time": "2019-11-26T21:23:18.873162Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End to end time: 860.2674803733826\n"
     ]
    }
   ],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the part_2 datasets to memory and de-cache part_1 datasets.\n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_7(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # step 1\n",
    "    dt = M.regression.DecisionTreeRegressor(labelCol='overall', maxDepth=5)\n",
    "    dt_model = dt.fit(train_data)\n",
    "    \n",
    "    # step 2\n",
    "    test_data_pred = dt_model.transform(test_data)\n",
    "    evaluator = M.evaluation.RegressionEvaluator(predictionCol='prediction',\n",
    "                                                labelCol='overall', metricName='rmse')\n",
    "    rmse = evaluator.evaluate(test_data_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['test_rmse'] = rmse\n",
    "    \n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_7')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': 0.8915823231982257}\n",
      "tests for task_7 --------------------------------------------------------------\n",
      "Test 1/1 : test_rmse ... Pass\n",
      "1/1 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_7(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_8(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    train, test = train_data.randomSplit([0.75, 0.25], seed=12345)\n",
    "    para_array = [5,7,9,12]\n",
    "    rmse_array = []\n",
    "    model_array = []\n",
    "    for x in para_array:\n",
    "        dt = M.regression.DecisionTreeRegressor(labelCol='overall', maxDepth=x)\n",
    "        dt_model = dt.fit(train)\n",
    "        test_data_pred = dt_model.transform(test)\n",
    "        evaluator = M.evaluation.RegressionEvaluator(predictionCol='prediction',\n",
    "                                                labelCol='overall', metricName='rmse')\n",
    "        rmse = evaluator.evaluate(test_data_pred)\n",
    "        rmse_array.append(rmse)\n",
    "        model_array.append(dt_model)\n",
    "    \n",
    "    # use the best\n",
    "    index = rmse_array.index(min(rmse_array))\n",
    "    best_model = model_array[index]\n",
    "    \n",
    "    test_data_pred = best_model.transform(test_data)\n",
    "    evaluator = M.evaluation.RegressionEvaluator(predictionCol='prediction',\n",
    "                                                labelCol='overall', metricName='rmse')\n",
    "    rmse_best = evaluator.evaluate(test_data_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None,\n",
    "        'valid_rmse_depth_5': None,\n",
    "        'valid_rmse_depth_7': None,\n",
    "        'valid_rmse_depth_9': None,\n",
    "        'valid_rmse_depth_12': None,\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res[\"test_rmse\"] =  rmse_best\n",
    "    res[\"valid_rmse_depth_5\"] = rmse_array[0]\n",
    "    res[\"valid_rmse_depth_7\"] = rmse_array[1]\n",
    "    res[\"valid_rmse_depth_9\"] = rmse_array[2]\n",
    "    res[\"valid_rmse_depth_12\"] = rmse_array[3]\n",
    "    \n",
    "    # TODO: delete this\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_8')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_rmse': 0.8657692186955247, 'valid_rmse_depth_5': 0.8891954350031208, 'valid_rmse_depth_7': 0.8734240833724545, 'valid_rmse_depth_9': 0.8677561873751738, 'valid_rmse_depth_12': 0.8654243192267065}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_8 --------------------------------------------------------------\n",
      "Test 1/5 : test_rmse ... Pass\n",
      "Test 2/5 : valid_rmse_depth_12 ... Pass\n",
      "Test 3/5 : valid_rmse_depth_5 ... Pass\n",
      "Test 4/5 : valid_rmse_depth_7 ... Pass\n",
      "Test 5/5 : valid_rmse_depth_9 ... Pass\n",
      "5/5 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_8(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End to end time: 1215.5420143604279\n"
     ]
    }
   ],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
